provider: zhipu
model: glm-4-long
label: glm-4-long
modelType: llm
features:
  - multi-tool-call
  - agent-thought
  - stream-tool-call
modelProperties:
  mode: chat
  contextSize: 10240
modelParameters:
  - name: temperature
    useTemplate: temperature
    defaultValue: 0.95
    min: 0.0
    max: 1.0
    help: 采样温度，控制输出的随机性，必须为正数取值范围是：(0.0,1.0]，不能等于 0,默认值为 0.95 值越大，会使输出更随机，更具创造性；值越小，输出会更加稳定或确定建议您根据应用场景调整 top_p 或 temperature 参数，但不要同时调整两个参数。
  - name: top_p
    useTemplate: top_p
    defaultValue: 0.7
    min: 0.0
    max: 1.0
    help: 用温度取样的另一种方法，称为核取样取值范围是：(0.0, 1.0) 开区间，不能等于 0 或 1，默认值为 0.7 模型考虑具有 top_p 概率质量tokens的结果例如：0.1 意味着模型解码器只考虑从前 10% 的概率的候选集中取 tokens 建议您根据应用场景调整 top_p 或 temperature 参数，但不要同时调整两个参数。
  - name: max_tokens
    useTemplate: max_tokens
    defaultValue: 1024
    min: 1
    max: 4096
